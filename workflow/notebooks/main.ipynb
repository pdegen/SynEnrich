{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import yaml\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "palette = sns.color_palette()\n",
    "pd.options.mode.copy_on_write = True\n",
    "scripts_dir = os.path.abspath(os.path.join(os.getcwd(), \"../scripts\"))\n",
    "sys.path.append(scripts_dir)\n",
    "workflows_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(workflows_dir)\n",
    "\n",
    "pretty_print = {\"string\": \"STRING\",\n",
    "      \"gseapy\": \"GSEApy\",\n",
    "      \"clusterProfiler\": \"ClusterProfiler\",\n",
    "      \"neg_signed_logpval\": \"signed logPValue\",\n",
    "      \"logFC\": \"logFC\",\n",
    "      \"s2n\":\"Signal-to-noise\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define params\n",
    "\n",
    "- `input_file`: Path (relative to project root) to a csv file containing a table with gene identifiers and ranking metric(s). Input files should be put in the `resources` folder.\n",
    "  \n",
    "- `project_name`: A string to tag output files. Results will be saved in `results/{project_name}/some_filename.{project_name}.csv`\n",
    "\n",
    "- `metrics`: A list of strings specifying columns in the input table that are used to rank the genes.\n",
    "\n",
    "- `libraries`: A list of libraries to be included. Currently supported: \"KEGG\", \"GO\", or path to .gmt file. If .gmt file is provided, `keytype_gmt` must be specified.\n",
    "\n",
    "- `tools`: A list of strings specifying the tools to be used. Currently supported : \"clusterProfiler\", \"gseapy\", \"string\". If results from [STRING](https://string-db.org/cgi/input?sessionId=b9myRH3ZDO2O&input_page_active_form=proteins_with_values) are to be used, the tsv files must be downloaded from the web tool and saved in the results folder (see \"Format STRING table\").\n",
    "  \n",
    "- `keytype`: String specifying the [type of gene identifier](https://www.bioconductor.org/help/course-materials/2014/useR2014/Integration.html) in the input file, e.g. \"ENSEMBLE\" or \"SYMBOL\".\n",
    "\n",
    "- `keytype_gmt`: (Optional) String specifying the type of gene identifier in the provided .gmt file (if .gmt file is provded in `libraries`).\n",
    "\n",
    "- `organismKEGG`: Currently supported: \"hsa\" for human and \"mmu\" for mouse.\n",
    "\n",
    "- `qval`: Q-value threshold to define significant terms. This value can be easily changed later on in downstream inspection of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User-defined variables ####\n",
    "\n",
    "input_file = \"resources/testdata/BRCA.edgerqlf.lfc0.csv\"\n",
    "project_name = \"BRCA.QLF\"\n",
    "\n",
    "metrics = ['logFC', 'neg_signed_logpval', 's2n']\n",
    "libraries = [\"KEGG\",\"GO\"]\n",
    "tools = [\"clusterProfiler\",\"gseapy\"]#,\"string\"]\n",
    "\n",
    "keytype = \"ENSEMBL\"\n",
    "keytype_gmt = \"\"\n",
    "organismKEGG = \"hsa\"\n",
    "qval = 0.05\n",
    "\n",
    "# Table with normalized expression values\n",
    "input_file_expression = \"../../resources/testdata/BRCA.csv\"\n",
    "control_tag = \"N\"\n",
    "case_tag = \"T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create config.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration dictionary\n",
    "config_data = {\n",
    "    'input_file': input_file,\n",
    "    'project_name': project_name,\n",
    "    'metrics': metrics,\n",
    "    'keytype': keytype,\n",
    "    'keytype_gmt': keytype_gmt if \"keytype_gmt\" in globals() else \"\",\n",
    "    'organismKEGG': organismKEGG,\n",
    "    'libraries': libraries,\n",
    "    'tools': tools,\n",
    "    'qval': qval,\n",
    "    'save_summary_dict': True\n",
    "}\n",
    "\n",
    "# Write to config.yaml\n",
    "config_filename = \"../../config/config.yaml\"\n",
    "with open(config_filename, 'w') as file:\n",
    "    yaml.dump(config_data, file, default_flow_style=False)\n",
    "\n",
    "print(f\"Configuration file '{config_filename}' created successfully!\")\n",
    "\n",
    "savepath = f\"../../results/{project_name}/\"\n",
    "figpath = f\"../../results/{project_name}/figures/\"\n",
    "!mkdir -p $figpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect/modify input\n",
    "\n",
    "This space can be used to calculate further ranking metrics that are missing in the input table. As an example, we read the output table from edgeR, calculate $-\\mathrm{sign}(\\log_2\\mathrm{FC})\\times\\log_{10}(p\\mathrm{-value})$, and add this as a new column to the table.\n",
    "\n",
    "**Careful:** Updating input files after jobs have been run will re-run the jobs the next time Snakemake is run. To prevent this, you can use `--touch` to update the timestamps of previously generated output files:\n",
    "\n",
    "`snakemake --touch --cores 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add negative signed log pval\n",
    "\n",
    "df = pd.read_csv(f\"../../{input_file}\", index_col=0)\n",
    "df[\"neg_signed_logpval\"] = -np.sign(df[\"logFC\"]) * np.log10(df[\"PValue\"])\n",
    "sig = df[df[\"FDR\"]<0.01]\n",
    "print(len(sig))\n",
    "\n",
    "# Optionally, if normalized count matrix provided, calculate signal-to-noise ratio\n",
    "\n",
    "def signal_to_noise(expr: pd.DataFrame, control: List[str], case: List[str]):\n",
    "    x1 = expr[control]\n",
    "    x2 = expr[case]\n",
    "    return (x2.mean(axis=1) - x1.mean(axis=1)) / (x1.std(axis=1) + x2.std(axis=1))\n",
    "\n",
    "if input_file_expression:\n",
    "    expr = pd.read_csv(input_file_expression, index_col=0)\n",
    "    control = [col for col in expr.columns if control_tag in col]\n",
    "    case = [col for col in expr.columns if case_tag in col]\n",
    "    s2n = signal_to_noise(expr, control, case)\n",
    "    df[\"s2n\"] = s2n.loc[df.index]\n",
    "\n",
    "\n",
    "display(df.sort_index().head())\n",
    "#df.to_csv(f\"../../{input_file}\") # Uncomment to save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "Inspect correlations between the two different ranking metrics (e.g. logFC and signed pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank1 = \"logFC\"\n",
    "rank2 = \"s2n\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "### Pearson correlation\n",
    "\n",
    "correlation = df[rank1].corr(df[rank2], method='pearson')\n",
    "sns.regplot(x=df[rank1], y=df[\"neg_signed_logpval\"], ax=ax[0], scatter_kws={'alpha':0.1}, line_kws={\"color\":palette[3]})\n",
    "ax[0].set_title(f\"Pearson: {correlation:.2f}\")\n",
    "\n",
    "ax[0].set(xlabel=rank1)\n",
    "ax[0].set(ylabel=f\"{rank2}\")\n",
    "\n",
    "### Spearman rank correlation\n",
    "\n",
    "df['rank1'] = df[rank1].rank(method='average')\n",
    "df['rank2'] = df[rank2].rank(method='average')\n",
    "rank_correlation = df['rank1'].corr(df['rank2'], method='spearman')\n",
    "\n",
    "sns.regplot(x=df['rank1'] ,y=df['rank2'], ax=ax[1], scatter_kws={'alpha':0.01}, line_kws={\"color\":palette[3]})\n",
    "ax[1].set_title(f\"Spearman: {rank_correlation:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[1].set(xlabel=f\"{rank1} [Rank]\")\n",
    "ax[1].set(ylabel=f\"{rank2} [Rank]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Snakemake\n",
    "\n",
    "Run the following command in project root directory:\n",
    "\n",
    "`snakemake --use-conda --cores 1` (adjust number of cores as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# cores = 1\n",
    "# command = f\"snakemake -s ../Snakefile --configfile ../../config/config.yaml --use-conda --cores {cores}\"\n",
    "# subprocess.run(command, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib\n",
    "from scripts.plots import npg_palette\n",
    "\n",
    "npg = npg_palette()\n",
    "output_files = glob.glob(f\"{savepath}/syn.*[tc]sv\")\n",
    "print(f\"Found {len(output_files)} output files:\\n\",*[o+\"\\n\" for o in output_files])\n",
    "\n",
    "summary_dict_file = f\"{savepath}/combined/syn.summary_dict.{project_name}.txt\"\n",
    "with open(summary_dict_file, \"rb\") as f:\n",
    "    summary_dict = pickle.load(f)\n",
    "\n",
    "print(\"summary_dict loaded\")\n",
    "summary_dict.keys()\n",
    "\n",
    "lib_names = {os.path.splitext(os.path.basename(l))[0]: l for l in libraries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case another qval threshold should be used\n",
    "# from scripts.combine_libs import create_summary_dict\n",
    "#summary_dict = create_summary_dict(f\"{savepath}/combined\",libraries,tools,metrics,project_name, qval=0.01, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection depth\n",
    "\n",
    "We count the number of unique analysis configuations (methods, rankings) each significant term appears in. We designate this as the *intersection depth* of a term. A depth of $N$ means that the corresponding enrichment term is significant in exactly $N$ configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = npg\n",
    "\n",
    "sns.set_theme(font_scale=1.2)\n",
    "\n",
    "nlib = len(lib_names.keys())\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    fig, axes = plt.subplots(2, nlib, figsize=(nlib*5,10))\n",
    "axes = axes.flatten()\n",
    "qval = 0.05\n",
    "\n",
    "### Intersection depth\n",
    "\n",
    "for ax, lib in zip(axes[:nlib],lib_names.keys()):\n",
    "    depth_df = summary_dict[lib][\"depth_df\"]\n",
    "    if len(depth_df) < 1:\n",
    "        print(f\"No terms found for {lib}\")\n",
    "        continue\n",
    "    h = sns.histplot(depth_df['Depth'], bins=depth_df['Depth'].max() - depth_df['Depth'].min() + 1, \n",
    "                discrete=True, ax=ax, alpha=1, color=npg[3])\n",
    "    h.bar_label(h.containers[0])\n",
    "    ax.set(title=lib, xlabel=\"Robustness\", ylabel=\"Terms\")\n",
    "    ax.set_xticks(range(depth_df['Depth'].min(), depth_df['Depth'].max() + 1))\n",
    "    ax.grid(axis='x')        \n",
    "    \n",
    "### Number of terms\n",
    "\n",
    "for ax, lib in zip(axes[nlib:], lib_names.keys()):\n",
    "    summary_df = summary_dict[lib][\"summary_df\"]\n",
    "    qv = summary_df.drop([\"Combined\",\"nan\"], axis=1, level=0)\n",
    "    qv = qv.xs(\"qvalue\", level=2, axis=1)\n",
    "    qv = qv.replace(np.nan,1)\n",
    "    qv = qv < qval\n",
    "\n",
    "    qqv=qv.sum().reset_index()\n",
    "    qqv.replace({\"Tool\": pretty_print, \"Metric\": pretty_print}, inplace=True)\n",
    "    qqv.index = qqv[\"Tool\"] + \"\\n\" + qqv[\"Metric\"]\n",
    "    qqv = qqv.drop([\"Tool\",\"Metric\"], axis=1)\n",
    "    qqv = qqv.sort_values(by=qqv.columns[0], ascending=False)\n",
    "\n",
    "    if ax == axes[nlib]:\n",
    "        hue_order = {qqv.index[i]: npg[i] for i in range(len(qqv))}\n",
    "\n",
    "    qqv[\"hue\"] = qqv.index\n",
    "    b = sns.barplot(data=qqv, x=qqv.index, y=qqv.iloc[:, 0], ax=ax, alpha=1, hue = 'hue', palette=palette, hue_order=hue_order)\n",
    "    for i in b.containers:\n",
    "        b.bar_label(i,)\n",
    "    ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=60, ha='right')\n",
    "    ax.set(title=lib, ylabel=\"Terms\", xlabel=None)\n",
    "    \n",
    "    # apply offset transform to all x ticklabels.\n",
    "    offset = matplotlib.transforms.ScaledTranslation(12/72., 3/72., fig.dpi_scale_trans)\n",
    "    for label in ax.xaxis.get_majorticklabels():\n",
    "        label.set_transform(label.get_transform() + offset)\n",
    "for i in range(len(axes)):\n",
    "    axes[i].annotate(chr(ord('A')+i), xy=(-0.08, 1.04), xycoords=\"axes fraction\", weight=\"bold\", va='center',ha='center', fontsize=18)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{figpath}/bars.{project_name}.pdf\")\n",
    "\n",
    "# include_bidirectional = True\n",
    "# s = summary_df.xs(\"Direction\",axis=1,level=2)\n",
    "# bidirectional_ix = s[(s == \"Both\").any(axis=1)].index\n",
    "# summary_df.loc[bidirectional_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venn diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.explore_results import get_sig_dict\n",
    "from scripts.plots import plot_venn\n",
    "\n",
    "fig, ax = plt.subplots(len(lib_names.keys()), len(metrics), figsize=(len(metrics)*4,len(lib_names.keys())*4))\n",
    "if len(lib_names.keys()) == 1: ax = np.expand_dims(ax, axis=1).T\n",
    "if len(metrics) == 1: ax = np.array([ax]).T\n",
    "\n",
    "for i, lib in enumerate(lib_names.keys()):\n",
    "    summary_df = summary_dict[lib][\"summary_df\"]\n",
    "    sig_dict = get_sig_dict(summary_df, tools, metrics, qval=0.05)\n",
    "    for j, metric in enumerate(metrics):\n",
    "        print(i,j)\n",
    "        plot_venn(sig_dict, tools, metric, ax[i][j], pretty_print)\n",
    "        ax[i][j].set_title(f\"{pretty_print[metric]} ({lib})\", fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{figpath}/venn.methodcomp.{project_name}.pdf\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(len(lib_names.keys()), 3, figsize=(12,len(lib_names.keys())*4))\n",
    "if len(lib_names.keys()) == 1: ax = np.expand_dims(ax, axis=1).T\n",
    "if len(metrics) == 1: ax = np.array([ax]).T\n",
    "\n",
    "for i, lib in enumerate(lib_names.keys()):\n",
    "    summary_df = summary_dict[lib][\"summary_df\"]\n",
    "    sig_dict = get_sig_dict(summary_df, tools, metrics, qval=0.05)\n",
    "    for j, tool in enumerate(tools):\n",
    "        plot_venn(sig_dict, tool, metrics, ax[i][j], pretty_print)\n",
    "        ax[i][j].set_title(f\"{pretty_print[tool]} ({lib})\", fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{figpath}/venn.metriccomp.{project_name}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpSet plots\n",
    "\n",
    "Useful for visualizing set intersection with more than 3 sets. \n",
    "\n",
    "https://upsetplot.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from upsetplot import from_memberships\n",
    "from upsetplot import UpSet\n",
    "\n",
    "for lib in lib_names.keys():\n",
    "\n",
    "    depth_df = summary_dict[lib][\"depth_df\"]\n",
    "    if len(depth_df) < 1:\n",
    "        print(f\"No terms found for {lib}\")\n",
    "        continue\n",
    "    memberships = depth_df[\"Configurations\"]\n",
    "    memberships_list = [categories.split(\" | \") for categories in memberships.values]\n",
    "    upset_ready = from_memberships(memberships_list)\n",
    "    upset_ready.index.names = [\" \".join([pretty_print[i] for i in u.split(\".\")]) for u in upset_ready.index.names] # pretty print\n",
    "\n",
    "    pd.options.mode.copy_on_write = False\n",
    "    UpSet(upset_ready, subset_size=\"count\", sort_by=\"cardinality\", show_counts=\"{:,}\").plot()\n",
    "    pd.options.mode.copy_on_write = True\n",
    "    pyplot.savefig(f\"{figpath}/upset.{lib}.{project_name}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lollipop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "def make_lollipop(df, lib, project_name, max_depth=0, ext=\"pdf\", title=None):\n",
    "\n",
    "    df[\"SignedDepth\"] = df[\"Depth\"] * df[\"Direction\"].apply(lambda x: 1 if x == \"Up\" else 0 if x == \"Both\" else -1)\n",
    "    df[\"logFDR\"] = -np.log10(df[\"Combined FDR\"])\n",
    "    ordered_df = df.sort_values(by=\"SignedDepth\")\n",
    "\n",
    "    my_range=range(1,len(df.index)+1)\n",
    "\n",
    "    max_label_length = max(len(label) for label in ordered_df[\"Description\"])\n",
    "    \n",
    "    with sns.axes_style(\"ticks\"):\n",
    "        fig_width = 4 + max_label_length * 0.08\n",
    "        fig_height = max(1.8,len(df)//3)\n",
    "        fig, ax = plt.subplots(1,1,figsize=(fig_width,fig_height))\n",
    "\n",
    "\n",
    "    ax.hlines(y=my_range, xmin=0, xmax=ordered_df[\"SignedDepth\"], zorder=98, color=\"grey\")\n",
    "    sns.scatterplot(data=ordered_df, x=\"SignedDepth\", y=range(1,1+len(ordered_df)), hue=\"logFDR\", ax=ax, zorder=99, s=100)\n",
    "\n",
    "    ax.set_yticks(my_range, ordered_df['Description'])\n",
    "\n",
    "    ax.set(title=f\"Top {lib} terms {project_name}\" if title == None else title)\n",
    "\n",
    "    ### COLOR BAR\n",
    "    cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "    norm = plt.Normalize(ordered_df['logFDR'].min(), ordered_df['logFDR'].max())\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "    # Remove the legend and add a colorbar\n",
    "    ax.get_legend().remove()\n",
    "    fig.colorbar(sm, cax=cax)\n",
    "    #clb.ax.set_title('This is a title')\n",
    "    fig.axes[1].set(title='-logFDR', xlabel='', ylabel='')\n",
    "\n",
    "\n",
    "    # some extra spacing top and bottom\n",
    "    ax.set_ylim(my_range[0]-1, my_range[-1]+1)\n",
    "\n",
    "    if max_depth:\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        #ax.set_xticks(range(-max_depth, max_depth+1))\n",
    "\n",
    "    if max_depth > 0:\n",
    "        low, high = ax.get_xlim()\n",
    "        if ordered_df[\"SignedDepth\"].max() > 0: high = max_depth + 0.5\n",
    "        if ordered_df[\"SignedDepth\"].min() < 0: low = -max_depth-0.5\n",
    "        ax.set_xlim(low,high)\n",
    "        if low == -max_depth-0.5 and high == max_depth + 0.5:\n",
    "            ax.axvline(0, ls=\"--\",color=\"grey\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"../../results/{project_name}/figures/lollipop.{lib}.{project_name}.{ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=1)\n",
    "\n",
    "for lib in lib_names.keys():\n",
    "\n",
    "    d = summary_dict[lib][\"depth_df\"]\n",
    "    if len(d) < 1:\n",
    "        print(f\"No terms found for {lib}\")\n",
    "        continue\n",
    "    if \"Direction\" not in d:\n",
    "        d[\"Direction\"] = d.index.str.split(\"_\").str[1].str.strip()\n",
    "    d.index = d.index.str.split(\"_\").str[0]\n",
    "    d.rename({\"Factors\":\"Configurations\"}, axis=1, inplace=True)\n",
    "    d[\"Combined FDR\"] = summary_dict[lib][\"summary_df\"].loc[d.index,(\"Combined\",\"nan\",\"Combined FDR\")]\n",
    "    d.sort_values(by=[\"Depth\",\"Combined FDR\"], ascending=[False,True], inplace=True)\n",
    "    outfile = f\"../../results/{project_name}/combined/syn.depth.{lib}.{project_name}.csv\"\n",
    "    d = d[[\"Description\",\"Depth\",\"Direction\",\"Combined FDR\",\"Configurations\"]]\n",
    "    d.to_csv(outfile)\n",
    "    \n",
    "    dd = d[(d[\"Depth\"]>1) & (d[\"Combined FDR\"]< 0.05)]\n",
    "    if len(dd) < 1:\n",
    "        print(f\"No terms found for {lib}\")\n",
    "        continue\n",
    "\n",
    "    title= f\"Top {lib} terms {project_name}\"\n",
    "    make_lollipop(dd.iloc[:30], lib, project_name, ext=\"pdf\", max_depth = 9, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-analysis\n",
    "\n",
    "Compare multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scripts.plots import npg_palette\n",
    "\n",
    "npg = npg_palette()\n",
    "\n",
    "#project_names = [\"BRCA.LRT\", \"Carmen.unpaired.LRT\", \"Chiara.KO_WT\", \"Chiara.SA_WT\", \"met.Exc7_DL.P90.p19rc\"]\n",
    "project_names = [\"THCA.QLF\",\"BRCA.QLF\", \"KIRC.QLF\", \"LIHC.QLF\",\n",
    "                 \"Carmen.paired.QLF\", \n",
    "                 \"Chiara.QLF.KO_WT\", \"Chiara.QLF.SA_WT\", \"Chiara.QLF.SD_WT\",\n",
    "                 \"met.Exc7_DL.P90.p19rc\", \"met.Inh_Sncg.P14.p19rc\"]\n",
    "\n",
    "pretty_datanames = {\"BRCA.QLF\":\"TCGA.BRCA.N-T\",\n",
    "                    \"THCA.QLF\":\"TCGA.THCA.N-T\",\n",
    "                    \"KIRC.QLF\":\"TCGA.KIRC.N-T\",\n",
    "                    \"LIHC.QLF\":\"TCGA.LIHC.N-T\",\n",
    "                    \"met.Exc7_DL.P90.p19rc\": \"sn.Exc7.P90.WT-SA\",\n",
    "                    \"met.Inh_Sncg.P14.p19rc\": \"sn.Inh.Sncg.P14.WT-SA\",\n",
    "                    \"Chiara.QLF.KO_WT\":\"Ser1016.WT-KO\",\n",
    "                    \"Chiara.QLF.SA_WT\":\"Ser1016.WT-SA\",\n",
    "                    \"Chiara.QLF.SD_WT\":\"Ser1016.WT-SD\",\n",
    "                    \"Carmen.paired.QLF\":\"CHK2.WT-KO\"}\n",
    "\n",
    "meta_dict = dict()\n",
    "for project in project_names:\n",
    "    try:\n",
    "        with open(f\"../../results/{project}/{project}.summary_dict.txt\", \"rb\") as f:\n",
    "            meta_dict[project] = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Project not found: {project}\")\n",
    "\n",
    "npg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_depth_dict = dict()\n",
    "\n",
    "for lib in libraries:\n",
    "    meta_depth_df = []\n",
    "    for project in meta_dict:\n",
    "        depth_df_p = meta_dict[project][lib][\"depth_df\"]\n",
    "        depth_df_p[\"Project\"] = project\n",
    "        depth_df_p.replace({\"Project\":pretty_datanames}, inplace=True)\n",
    "        meta_depth_df.append(depth_df_p)\n",
    "\n",
    "    meta_depth_df = pd.concat(meta_depth_df)\n",
    "    meta_depth_dict[lib] = meta_depth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_summary_dict = dict()\n",
    "\n",
    "for lib in libraries:\n",
    "    meta_summary_df = []\n",
    "    for project in meta_dict:\n",
    "        df = meta_dict[project][lib][\"summary_df\"]\n",
    "        df.index = df.index + \".\" + project # needed for venn\n",
    "        meta_summary_df.append(df)\n",
    "\n",
    "    meta_summary_df = pd.concat(meta_summary_df)\n",
    "    meta_summary_dict[lib] = meta_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "palette = npg\n",
    "\n",
    "sns.set(font_scale=1.2)\n",
    "#sns.set_style(\"whitegrid\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "### Intersection depth\n",
    "\n",
    "for ax, lib in zip(axes[:2],meta_depth_dict):\n",
    "    sns.despine()\n",
    "    depth_df = meta_depth_dict[lib]\n",
    "    h = sns.histplot(depth_df['Depth'], bins=depth_df['Depth'].max() - depth_df['Depth'].min() + 1, \n",
    "                discrete=True, ax=ax, alpha=1, color=npg[3], label=\"Non-TCGA\")\n",
    "    h.bar_label(h.containers[0])\n",
    "    ax.set(title=lib, xlabel=\"Intersection Depth\", ylabel=\"Terms\")\n",
    "    ax.set_xticks(range(depth_df['Depth'].min(), depth_df['Depth'].max() + 1))\n",
    "    ax.grid(axis='x')\n",
    "\n",
    "    ### plot TCGA only on top\n",
    "    TCGA = depth_df[depth_df[\"Project\"].str.startswith(\"TCGA\")]\n",
    "    sns.histplot(TCGA['Depth'], bins=depth_df['Depth'].max() - depth_df['Depth'].min() + 1, \n",
    "                discrete=True, ax=ax, alpha=1, color=npg[5], label=\"TCGA\")\n",
    "    \n",
    "    ### label bars    \n",
    "    nonTCGA = depth_df[~depth_df[\"Project\"].str.startswith(\"TCGA\")]\n",
    "    nonTCGA_counts = nonTCGA['Depth'].value_counts().sort_index()\n",
    "    TCGA_counts = TCGA['Depth'].value_counts().sort_index()\n",
    "    for i in range(depth_df[\"Depth\"].max()):\n",
    "        ax.text(s=TCGA_counts.iloc[i], x=i+1,y=TCGA_counts.iloc[i]/2,  va='center',ha='center')\n",
    "        ax.text(s=nonTCGA_counts.iloc[i], x=i+1,y=TCGA_counts.iloc[i]+nonTCGA_counts.iloc[i]/2,  va='center',ha='center',color=\"white\")\n",
    "\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(False)\n",
    "\n",
    "    if ax == axes[0]:\n",
    "        pass#ax.set_ylim(0,349)\n",
    "\n",
    "### Number of terms\n",
    "\n",
    "for ax, lib in zip(axes[2:], meta_summary_dict):\n",
    "    sns.despine()\n",
    "    summary_df = meta_summary_dict[lib]\n",
    "    qv = summary_df.drop([\"Combined\",\"nan\"], axis=1, level=0)\n",
    "    qv = qv.xs(\"qvalue\", level=2, axis=1)\n",
    "    qv = qv.replace(np.nan,1)\n",
    "    qv = qv < qval\n",
    "\n",
    "    qqv=qv.sum().reset_index()\n",
    "    qqv.replace({\"Tool\": pretty_print, \"Metric\": pretty_print}, inplace=True)\n",
    "    qqv.index = qqv[\"Tool\"] + \"\\n\" + qqv[\"Metric\"]\n",
    "    qqv = qqv.drop([\"Tool\",\"Metric\"], axis=1)\n",
    "    qqv = qqv.sort_values(by=qqv.columns[0], ascending=False)\n",
    "\n",
    "    if ax == axes[2]:\n",
    "        hue_order = {qqv.index[i]: npg[i] for i in range(len(qqv))}\n",
    "\n",
    "    qqv[\"hue\"] = qqv.index\n",
    "    b = sns.barplot(data=qqv, x=qqv.index, y=qqv.iloc[:, 0], ax=ax, alpha=1, hue = 'hue', palette=palette, hue_order=hue_order)\n",
    "    for i in b.containers:\n",
    "        b.bar_label(i,)\n",
    "    ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=60, ha='right')\n",
    "    ax.set(title=lib, ylabel=\"Terms\", xlabel=None)\n",
    "\n",
    "    # apply offset transform to all x ticklabels.\n",
    "    offset = matplotlib.transforms.ScaledTranslation(12/72., 3/72., fig.dpi_scale_trans)\n",
    "    for label in ax.xaxis.get_majorticklabels():\n",
    "        label.set_transform(label.get_transform() + offset)\n",
    "        \n",
    "for i in range(len(axes)):\n",
    "    axes[i].annotate(chr(ord('A')+i), xy=(-0.08, 1.04), xycoords=\"axes fraction\", weight=\"bold\", va='center',ha='center', fontsize=18)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../../results/meta/bars.meta.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.plots import plot_venn\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "\n",
    "for i, lib in enumerate(libraries):\n",
    "    summary_df = meta_summary_dict[lib]\n",
    "    sig_dict = get_sig_dict(summary_df, tools, metrics, qval=0.05)\n",
    "    for j, metric in enumerate(metrics):\n",
    "        plot_venn(sig_dict, tools, metric, ax[i][j], pretty_print)\n",
    "        ax[i][j].set_title(f\"{pretty_print[metric]} ({lib})\", fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../../results/meta/venn.methodcomp.meta.pdf\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12,8))\n",
    "\n",
    "for i, lib in enumerate(libraries):\n",
    "    summary_df = meta_summary_dict[lib]\n",
    "    sig_dict = get_sig_dict(summary_df, tools, metrics, qval=0.05)\n",
    "    for j, tool in enumerate(tools):\n",
    "        plot_venn(sig_dict, tool, metrics, ax[i][j], pretty_print)\n",
    "        ax[i][j].set_title(f\"{pretty_print[tool]} ({lib})\", fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../../results/meta/venn.metriccomp.meta.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from upsetplot import from_memberships, UpSet\n",
    "\n",
    "for lib in libraries:\n",
    "\n",
    "    depth_df = meta_depth_dict[lib]\n",
    "    memberships = depth_df[\"Factors\"]\n",
    "    memberships_list = [categories.split(\" | \") for categories in memberships.values]\n",
    "    upset_ready = from_memberships(memberships_list)\n",
    "    upset_ready.index.names = [\" \".join([pretty_print[i] for i in u.split(\".\")]) for u in upset_ready.index.names] # pretty print\n",
    "\n",
    "    pd.options.mode.copy_on_write = False\n",
    "    UpSet(upset_ready, subset_size=\"count\", sort_by=\"cardinality\", show_counts=\"{:,}\", orientation=\"vertical\").plot()\n",
    "    pd.options.mode.copy_on_write = True\n",
    "    pyplot.savefig(f\"../../results/meta/upset.{lib}.meta.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.despine()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "### Number of terms per dataset\n",
    "\n",
    "for ax, lib in zip(axes[:2],libraries):\n",
    "    sns.despine()\n",
    "    depth_df = meta_depth_dict[lib]\n",
    "    depth_df.replace({\"Project\":pretty_datanames}, inplace=True)\n",
    "\n",
    "    counts = depth_df.groupby([\"Project\"]).count()[\"Factors\"].sort_values(ascending=False)\n",
    "\n",
    "    counts = pd.DataFrame(counts)\n",
    "\n",
    "    counts[\"hue\"] = counts.index\n",
    "    #counts.replace({\"hue\":pretty_datanames}, inplace=True)\n",
    "\n",
    "    if ax == axes[0]:\n",
    "        hue_order = {counts.iloc[i][\"hue\"]: npg[i] for i in range(len(counts))}\n",
    "        \n",
    "    b = sns.barplot(data=counts, y=\"Factors\", x=\"hue\", ax=ax, hue=\"hue\", hue_order=hue_order, palette=npg[:len(project_names)])\n",
    "    ax.set(title=lib,xlabel=None,ylabel=\"Combined Terms\")\n",
    "\n",
    "    for i in b.containers:\n",
    "        b.bar_label(i,)\n",
    "\n",
    "    if ax == axes[1]:\n",
    "        ax.set_ylim(0,2999)\n",
    "    \n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylim(0,199)\n",
    "\n",
    "    # mark depths with horzontal lines\n",
    "    g = depth_df.groupby([\"Project\"])[\"Depth\"].value_counts()\n",
    "    for j, p in enumerate(counts.index):\n",
    "        sum = 0\n",
    "        prev_sum = 0\n",
    "        maxdepth = g.index.get_level_values(\"Depth\").max()\n",
    "        for i in range(1,1+maxdepth):\n",
    "            try:\n",
    "                sum += g.loc[(p,i)]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            if i < maxdepth:\n",
    "                ax.scatter(j, sum, marker=\"_\", color=\"black\", s=550, alpha=0.5)\n",
    "            if j == 0:\n",
    "                ax.text(s=f\"d{i}\",x=j,y=prev_sum + 0.5*(sum-prev_sum), ha=\"center\", va=\"center\",fontsize=10)\n",
    "            prev_sum = sum\n",
    "\n",
    "for ax, lib in zip(axes[2:], libraries):\n",
    "    depth_df = meta_depth_dict[lib]\n",
    "    #depth_df.replace({\"Project\":pretty_datanames}, inplace=True)\n",
    "    #sns.barplot(data=depth_df, x=\"Project\", y=\"Depth\", hue=\"Project\", errorbar=\"sd\", ax=ax, order=hue_order.keys(), hue_order=hue_order, palette=npg[:len(project_names)])\n",
    "    sns.boxplot(data=depth_df, x=\"Project\", y=\"Depth\", hue=\"Project\", ax=ax, order=hue_order.keys(), hue_order=hue_order, palette=npg[:len(project_names)])\n",
    "    ax.set(ylabel=\"Enrichment Depth\",title=lib)\n",
    "\n",
    "    if ax == axes[3]:\n",
    "        ax.set_ylim(axes[2].get_ylim())\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    axes[i].annotate(chr(ord('A')+i), xy=(-0.08, 1.04), xycoords=\"axes fraction\", weight=\"bold\", va='center',ha='center', fontsize=18)\n",
    "    axes[i].set_xticks(axes[i].get_xticks(), axes[i].get_xticklabels(), rotation=30, ha='right')\n",
    "    axes[i].set(xlabel=None)\n",
    "\n",
    "    # annotate KI\n",
    "    axes[i].axvline(len(pretty_datanames)-4.5,0,axes[i].get_ylim()[1],ls=\"--\",color=\"black\",alpha=0.7)\n",
    "    #axes[i].annotate(\"KI\", xy=(0,0), xytext=(len(pretty_datanames)-3, 0.8*axes[i].get_ylim()[1] ), xycoords=\"data\",zorder=99)\n",
    "\n",
    "    if i == 0:\n",
    "        axes[i].annotate(\"KI\", xy=(0,0), xytext=(0.78,0.8), xycoords=\"axes fraction\",zorder=99)\n",
    "    elif i == 1:\n",
    "        axes[i].annotate(\"KI\", xy=(0,0), xytext=(0.78,0.735), xycoords=\"axes fraction\",zorder=99)\n",
    "    elif i == 2:\n",
    "        axes[i].annotate(\"KI\", xy=(0,0), xytext=(0.78,0.84), xycoords=\"axes fraction\",zorder=99)\n",
    "    else:\n",
    "        axes[i].annotate(\"KI\", xy=(0,0), xytext=(0.78,0.84), xycoords=\"axes fraction\",zorder=99)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../../results/meta/bars.data.meta.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRING\n",
    "\n",
    "### Format STRING table\n",
    "\n",
    "User can (optionally) manually add [STRING](https://string-db.org/cgi/input?sessionId=b9myRH3ZDO2O&input_page_active_form=proteins_with_values) functional scoring output tables to the results folder, and they will be combined with the output from SynEnrich. For this, STRING tables have to be formatted first:\n",
    "\n",
    "1) Download the .tsv file from the STRING web tool and save it in `results/{project_name}`.\n",
    "\n",
    "2) Rename the file to `syn.string.{metric}.{project_name}.tsv` where {metric} is to be replaced with the ranking metric (e.g. logFC) and {project_name} with the project name specified at the beginning of this notebook.\n",
    "\n",
    "3) Use the cell below to format the STRING table (which will split the file in separate KEGG and GO tables and rename some columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import format_string_table\n",
    "\n",
    "string_file1 = f\"{savepath}/syn.string.logFC.{project_name}.tsv\"\n",
    "string_file2 = f\"{savepath}/syn.string.neg_signed_logpval.{project_name}.tsv\"\n",
    "string_file3 = f\"{savepath}/syn.string.s2n.{project_name}.tsv\"\n",
    "\n",
    "string_files = [string_file1, string_file2, string_file3]\n",
    "\n",
    "for string_file in string_files:\n",
    "\n",
    "    string = pd.read_csv(string_file, index_col=0, sep=\"\\t\")\n",
    "\n",
    "    for library in [\"GO\", \"KEGG\"]:\n",
    "        string_formatted = format_string_table(string, library)\n",
    "        #display(string_formatted.head())\n",
    "        string_formatted.to_csv(string_file.replace(f\"{project_name}.tsv\", f\"{library}.{project_name}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare STRING upload\n",
    "\n",
    "The cell below can be used to split the input table into several files for each ranking metric, which is convenient for uploading to STRING.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../\"+input_file, index_col=0)\n",
    "p = \"../../\" + \"/\".join(input_file.split(\"/\")[:-1])\n",
    "name = input_file.split(\"/\")[-1]\n",
    "for metric in metrics:\n",
    "    print(len(df[metric]))\n",
    "    df[metric] = df[metric].dropna()\n",
    "    print(len(dff))\n",
    "    dff.to_csv(os.path.join(p,f\"string.input.{metric}.\"+name), header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crreate STRING gmt file\n",
    "\n",
    "The cell below can be used to format a txt file with enrichment terms [downloaded from STRING](https://string-db.org/cgi/download?sessionId=b26FUyLIdNfn) into a `.gmt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts import combine_libs\n",
    "import scripts.utils\n",
    "import importlib\n",
    "importlib.reload(scripts.utils)\n",
    "\n",
    "from scripts.utils import create_string_gmt\n",
    "\n",
    "#infile = \"../../resources/Ontologies/9606.protein.enrichment.terms.v12.0.tsv\" #human \n",
    "infile = \"../../resources/Ontologies/10090.protein.enrichment.terms.v12.0.txt\" #mouse\n",
    "\n",
    "orgid = os.path.basename(infile).split(\".\")[0]\n",
    "orgid_dict = {\"9606\": \"human\", \"10090\": \"mouse\"}\n",
    "species = orgid_dict[orgid]\n",
    "print(orgid, species)\n",
    "\n",
    "outfile = f'../../resources/Ontologies/GO_STRING_{species}.gmt'\n",
    "\n",
    "create_string_gmt(infile, outfile, orgid, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SynEnrich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
