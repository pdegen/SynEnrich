{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for misc. work on personal projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "scripts_dir = os.path.abspath(os.path.join(os.getcwd(), \"../scripts\"))\n",
    "sys.path.append(scripts_dir)\n",
    "workflows_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(workflows_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "source(\"../../.Rprofile\")\n",
    ".libPaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User-defined variables ####\n",
    "\n",
    "input_file = \"resources/Carmen/edger.paired.qlf.lfc0.csv\"\n",
    "project_name = \"Carmen.paired.QLF\"\n",
    "\n",
    "input_file = \"resources/Chiara/edger.qlf.lfc0.KO_WT.p1.csv\"\n",
    "project_name = \"Chiara.QLF.KO_WT\"\n",
    "\n",
    "input_file = \"resources/Chiara/edger.qlf.lfc0.SA_WT.p1.csv\"\n",
    "project_name = \"Chiara.QLF.SA_WT\"\n",
    "\n",
    "input_file = \"resources/Chiara/edger.qlf.lfc0.SD_WT.p1.csv\"\n",
    "project_name = \"Chiara.QLF.SD_WT\"\n",
    "\n",
    "input_file = \"resources/Chiara/edger.qlf.lfc0.KO_SD.p1.csv\"\n",
    "project_name = \"Chiara.QLF.KO_SD\"\n",
    "\n",
    "input_file = \"resources/Chiara/edger.qlf.lfc0.KO_SA.p1.csv\"\n",
    "project_name = \"Chiara.QLF.KO_SA\"\n",
    "\n",
    "input_file = \"resources/Chiara/edger.qlf.lfc0.SD_SA.p1.csv\"\n",
    "project_name = \"Chiara.QLF.SD_SA\"\n",
    "\n",
    "# input_file = \"resources/Chiara/permuted.edger.qlf.lfc0.KO_SA.p1.csv\"\n",
    "# project_name = \"permuted.Chiara.QLF.SD_SA\"\n",
    "\n",
    "# input_file = \"resources/TCGA/BRCA.edgerqlf.lfc0.csv\"\n",
    "# project_name = \"BRCA.QLF\"\n",
    "\n",
    "# input_file = \"resources/TCGA/THCA.edgerqlf.lfc0.csv\"\n",
    "# project_name = \"THCA.QLF\"\n",
    "\n",
    "# input_file = \"resources/TCGA/KIRC.edgerqlf.lfc0.csv\"\n",
    "# project_name = \"KIRC.QLF\"\n",
    "\n",
    "# input_file = \"resources/TCGA/LIHC.edgerqlf.lfc0.csv\"\n",
    "# project_name = \"LIHC.QLF\"\n",
    "\n",
    "\n",
    "metrics = ['logFC', 'neg_signed_logpval']\n",
    "libraries = [\"KEGG\",\"GO\"]\n",
    "tools = [\"clusterProfiler\",\"gseapy\",\"string\"]\n",
    "\n",
    "# ClusterProfiler\n",
    "\n",
    "keytype = \"ENSEMBL\"\n",
    "organismKEGG = \"hsa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User-defined variables ####\n",
    "\n",
    "input_file = \"resources/Liana/deg.edger.lrt.batch.unm_0.6.clean.clExc7_DL.thresh.0.2.2024-01-22-17-42.P90.p19rc.csv\"\n",
    "project_name = \"met.Exc7_DL.P90.p19rc\"\n",
    "\n",
    "\n",
    "input_file = \"resources/Liana/deg.edger.lrt.batch.unm_0.6.clean.clInh_Sncg.thresh.0.2.2024-01-19-11-47.P14.p19rc.csv\"\n",
    "project_name = \"met.Inh_Meis2.P14.string\"\n",
    "\n",
    "\n",
    "metrics = ['logFC', 'neg_signed_logpval']\n",
    "libraries = [\"KEGG\",\"GO\"]\n",
    "tools = [\"clusterProfiler\",\"gseapy\",\"string\"]\n",
    "\n",
    "# ClusterProfiler\n",
    "\n",
    "keytype = \"SYMBOL\"\n",
    "organismKEGG = \"mmu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move met files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_names = [project_name]\n",
    "\n",
    "#project_names = [\"Chiara.QLF.KO_WT\",\"Chiara.QLF.SA_WT\",\"Chiara.QLF.SD_WT\",\"Chiara.QLF.KO_SD\",\"Chiara.QLF.KO_SA\",\"Chiara.QLF.SD_SA\"]\n",
    "\n",
    "project_names = [\n",
    "    \"met.Astrocytes1.P14\",\n",
    "    \"met.Astrocytes2.P14\",\n",
    "    \"met.COPs.P14\",\n",
    "    \"met.Endothelial.P14\",\n",
    "    \"met.Exc1_SL.P14\",\n",
    "    \"met.Exc2_ML.P14\",\n",
    "    \"met.Exc3_ML.P14\",\n",
    "    \"met.Exc4_ML.P14\",\n",
    "    \"met.Exc5_ML.P14\",\n",
    "    \"met.Exc6_ML.P14\",\n",
    "    \"met.Exc7_DL.P14\",\n",
    "    \"met.Exc8_DL.P14\",\n",
    "    \"met.Exc9_DL.P14\",\n",
    "    \"met.Inh_Lamp5.P14\",\n",
    "    \"met.Inh_Meis2.P14\",\n",
    "    \"met.Inh_Pvalb.P14\",\n",
    "    \"met.Inh_Vip.P14\",\n",
    "    \"met.MFO.P14\",\n",
    "    \"met.Microglia.P14\",\n",
    "    \"met.OPC.P14\",\n",
    "\n",
    "    \"met.Astrocytes1.P90\",\n",
    "    \"met.Endothelial.P90\",\n",
    "    \"met.Exc1_SL.P90\",\n",
    "    \"met.Exc2_ML.P90\",\n",
    "    \"met.Exc3_ML.P90\",\n",
    "    \"met.Exc6_ML.P90\",\n",
    "    \"met.Exc7_DL.P90\",\n",
    "    \"met.Exc8_DL.P90\",\n",
    "    \"met.Exc9_DL.P90\",\n",
    "    \"met.Inh_Lamp5.P90\",\n",
    "    \"met.Inh_Meis2.P90\",\n",
    "    \"met.Inh_Pvalb.P90\",\n",
    "    \"met.Inh_Vip.P90\",\n",
    "    \"met.MFO.P90\",\n",
    "    \"met.Microglia.P14\",\n",
    "    \"met.OPC.P90\"\n",
    "]\n",
    "\n",
    "new = [\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all clsuter files to target dir\n",
    "import os\n",
    "import glob\n",
    "\n",
    "do_copy = True\n",
    "latest_only = False\n",
    "new_only = False\n",
    "\n",
    "if do_copy:\n",
    "    os.chdir(\"/home/peter/Work/OmicsPhD/SynEnrich/workflow/notebooks\")\n",
    "\n",
    "    for pr in project_names:\n",
    "        if latest_only and pr != project_names[-1]: continue\n",
    "        if new_only and pr not in new: continue\n",
    "        p = f\"../../results/{pr}/\"\n",
    "        comb = glob.glob(f\"{p}/combined/*depth.GO*\")\n",
    "        #figs = glob.glob(f\"{p}/figures/*lollipop*.GO_STRING*png\")\n",
    "        #figs += glob.glob(f\"{p}/figures/*heat*png\")\n",
    "        ff = comb #figs\n",
    "        for f in ff:\n",
    "            subfolder = \"csv\" if \"csv\" in f else \"figs\"\n",
    "            if subfolder == \"figs\":\n",
    "                if \"lollipop\" in f:\n",
    "                    subfolder += \"/lollipop_depth7_split_subontology\"\n",
    "            #     elif \"venn\" in f:\n",
    "            #         subfolder += \"/venn\"\n",
    "            #     elif \"upset\" in f:\n",
    "            #         subfolder += \"/upset\"\n",
    "            #     elif \"bars\" in f:\n",
    "            #         subfolder += \"/bars\"\n",
    "            print(f)\n",
    "            os.system(f\"mkdir -p ../../results/Liana_all/{subfolder}\")\n",
    "            os.system(f\"cp {f} ../../results/Liana_all/{subfolder}\")\n",
    "\n",
    "else:\n",
    "    print(\"Nothing done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_names_dict = {p: False for p in project_names}\n",
    "project_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update project_name/config.yaml using config/config.yaml\n",
    "import yaml\n",
    "os.chdir(\"/home/peter/Work/OmicsPhD/SynEnrich/\")\n",
    "for pr in project_names_dict:\n",
    "    print(pr)\n",
    "    config_file_path_pr = f\"results/{pr}/config.yaml\"\n",
    "    with open(config_file_path_pr, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "        infile_pr = config[\"input_file\"]\n",
    "\n",
    "    config_file_path = \"config/config.yaml\"\n",
    "    with open(config_file_path, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "    config[\"input_file\"] = infile_pr\n",
    "    config[\"project_name\"] = pr\n",
    "\n",
    "    with open(config_file_path_pr, 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_snakemake(cores=1, touch=False, dry=False) -> bool:\n",
    "    # Call Snakemake from the notebook\n",
    "    command = [\n",
    "        \"snakemake\",\n",
    "        \"--cores\", str(cores),          # Number of cores to use\n",
    "        \"--use-conda\"                   # Use conda if required\n",
    "    ] if not touch else [\"snakemake\", \"--touch\"]\n",
    "\n",
    "    if not touch and dry: \n",
    "        command += [\"-n\"]\n",
    "    \n",
    "    # Run the Snakemake workflow and wait for it to complete\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    print(command)\n",
    "    # Print the output for debugging\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "\n",
    "    success = \"At least one job did not complete successfully.\" not in result.stderr\n",
    "    return success\n",
    "\n",
    "do_rerun = False\n",
    "cores = 3\n",
    "os.chdir(\"/home/peter/Work/OmicsPhD/SynEnrich/\")\n",
    "\n",
    "if do_rerun:\n",
    "    for pr in project_names_dict.keys():\n",
    "        p = f\"results/{pr}/\"\n",
    "        print(p)\n",
    "\n",
    "        if project_names_dict[pr]:\n",
    "            print(f\"{pr} succeeded, skipping\")\n",
    "            continue\n",
    "        \n",
    "        run_snakemake(cores=4, touch=True)\n",
    "        os.system(f\"cp {p}/config.yaml config/config.yaml\")\n",
    "        #os.system(f\" rm {p}/combined/*\")\n",
    "        os.system(f\" rm {p}/figures/*\")\n",
    "        success = run_snakemake(cores=4, dry=False)\n",
    "\n",
    "        project_names_dict[pr] = success\n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "succeeded = sum(project_names_dict.values())\n",
    "print(f\"{succeeded} succeeded out of {len(project_names_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succeeded = sum(project_names_dict.values())\n",
    "print(f\"{succeeded} succeeded out of {len(project_names_dict.keys())}\")\n",
    "project_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"../../results/met.Astrocytes1.P90/syn.clusterProfiler.neg_signed_logpval.GO_STRING_mouse.met.Astrocytes1.P90.csv\"\n",
    "tab = pd.read_csv(f, index_col=0)\n",
    "sig = tab[tab[\"qvalue\"]<80.05]\n",
    "print(len(sig))\n",
    "sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for obsolete GO terms\n",
    "\n",
    "Use goatools env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goatools.base import get_godag\n",
    "from goatools.obo_parser import GODag\n",
    "\n",
    "def fetch_go_info(go_terms):\n",
    "\n",
    "    if len(go_terms) < 1:\n",
    "        print(\"No terms passed, returning\")\n",
    "        return\n",
    "    \n",
    "    # Download the latest GO DAG (Gene Ontology Directed Acyclic Graph) file if not already available\n",
    "    godag = get_godag(\"go-basic.obo\")#, optional_attrs={'consider', 'replaced_by', 'comment'})\n",
    "    \n",
    "    # Check if the term exists in the GO DAG\n",
    "    go_dict = {term: None for term in go_terms}\n",
    "    for go_term in go_dict:\n",
    "        if go_term in godag:\n",
    "            term = godag[go_term]\n",
    "            info = {\n",
    "                \"Name\": term.name,\n",
    "                #\"Namespace\": term.namespace,\n",
    "                \"Obsolete\": term.is_obsolete,\n",
    "                #\"Consider\": term.consider if term.consider else \"None\",\n",
    "                #\"Replaced By\": term.replaced_by if term.replaced_by else \"None\",\n",
    "                #\"Comment\": term.comment if term.comment else \"None\"\n",
    "            }\n",
    "            go_dict[go_term] = info\n",
    "\n",
    "    df = pd.DataFrame(go_dict).T\n",
    "    df[\"Obsolete\"].fillna(True, inplace=True)\n",
    "    return df\n",
    "\n",
    "go_terms = [\"GO:0098936\", \"GO:0045211\"]\n",
    "go_info = fetch_go_info(go_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_names_dict = {pr: False for pr in project_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all clsuter files to target dir\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "do_it = False\n",
    "latest_only = False\n",
    "new_only = False\n",
    "\n",
    "if do_it:\n",
    "    for pr in project_names:\n",
    "        if latest_only and pr != project_names[-1]:\n",
    "            continue\n",
    "        if new_only and pr not in new:\n",
    "            continue\n",
    "\n",
    "        if project_names_dict[pr]:\n",
    "            continue\n",
    "\n",
    "        p = f\"../../results/{pr}/\"\n",
    "        f = f\"{p}/combined/syn.depth.GO_STRING_mouse.{pr}.csv\"\n",
    "        depth = pd.read_csv(f, index_col=0)\n",
    "\n",
    "        if \"IsObsolete\" in depth:\n",
    "            project_names_dict[pr] = True\n",
    "            continue\n",
    "\n",
    "        if str(depth.iloc[0,0]).startswith(\"No terms found\"):\n",
    "            continue\n",
    "        \n",
    "        print(pr)\n",
    "        terms = list(depth.index)\n",
    "        go_info = fetch_go_info(terms)\n",
    "        depth[\"IsObsolete\"] = go_info.loc[depth.index,\"Obsolete\"]\n",
    "        cols = list(depth.columns[:-3]) + [\"IsObsolete\",\"Configurations\",\"Genes\"]\n",
    "        depth = depth[cols]\n",
    "        depth.rename({\"Depth\": \"Robustness\", \"Combined FDR\":\"Mean p.adj\",\"ONTOLOGY\":\"Ontology\",\"Configurations\":\"Pipelines\"}, axis=1, inplace=True)\n",
    "        depth.to_csv(f)\n",
    "        project_names_dict[pr] = True\n",
    "\n",
    "else:\n",
    "    print(\"Nothing done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lollipop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "def make_lollipop(df, lib, project_name, max_depth=0, ext=\"pdf\", title=None):\n",
    "\n",
    "    df[\"SignedDepth\"] = df[\"Depth\"] * df[\"Direction\"].apply(lambda x: 1 if x == \"Up\" else 0 if x == \"Both\" else -1)\n",
    "    df[\"logFDR\"] = -np.log10(df[\"Combined FDR\"])\n",
    "    ordered_df = df.sort_values(by=\"SignedDepth\")\n",
    "\n",
    "    my_range=range(1,len(df.index)+1)\n",
    "\n",
    "    max_label_length = max(len(label) for label in ordered_df[\"Description\"])\n",
    "    \n",
    "    with sns.axes_style(\"ticks\"):\n",
    "        fig_width = 4 + max_label_length * 0.08\n",
    "        fig_height = max(1.8,len(df)//3)\n",
    "        fig, ax = plt.subplots(1,1,figsize=(fig_width,fig_height))\n",
    "\n",
    "\n",
    "    ax.hlines(y=my_range, xmin=0, xmax=ordered_df[\"SignedDepth\"], zorder=98, color=\"grey\")\n",
    "    sns.scatterplot(data=ordered_df, x=\"SignedDepth\", y=range(1,1+len(ordered_df)), hue=\"logFDR\", ax=ax, zorder=99, s=100)\n",
    "\n",
    "    ax.set_yticks(my_range, ordered_df['Description'])\n",
    "\n",
    "    ax.set(title=f\"Top {lib} terms {project_name}\" if title == None else title)\n",
    "\n",
    "    ### COLOR BAR\n",
    "    cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "    norm = plt.Normalize(ordered_df['logFDR'].min(), ordered_df['logFDR'].max())\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "    # Remove the legend and add a colorbar\n",
    "    ax.get_legend().remove()\n",
    "    fig.colorbar(sm, cax=cax)\n",
    "    #clb.ax.set_title('This is a title')\n",
    "    fig.axes[1].set(title='-logFDR', xlabel='', ylabel='')\n",
    "\n",
    "\n",
    "    # some extra spacing top and bottom\n",
    "    ax.set_ylim(my_range[0]-1, my_range[-1]+1)\n",
    "\n",
    "    if max_depth:\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        #ax.set_xticks(range(-max_depth, max_depth+1))\n",
    "\n",
    "    if max_depth > 0:\n",
    "        low, high = ax.get_xlim()\n",
    "        if ordered_df[\"SignedDepth\"].max() > 0: high = max_depth + 0.5\n",
    "        if ordered_df[\"SignedDepth\"].min() < 0: low = -max_depth-0.5\n",
    "        ax.set_xlim(low,high)\n",
    "        if low == -max_depth-0.5 and high == max_depth + 0.5:\n",
    "            ax.axvline(0, ls=\"--\",color=\"grey\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"../../results/{project_name}/figures/lollipop.{lib}.{project_name}.{ext}\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "libs = [\"KEGG\", \"GO\"]\n",
    "\n",
    "for project_name in project_names:\n",
    "    \n",
    "    #if \"SD_WT\" not in project_name: continue\n",
    "    \n",
    "    ff = f\"../../results/{project_name}/combined/syn.summary_dict.{project_name}.txt\"\n",
    "    with open(ff, \"rb\") as f:\n",
    "        summary_dict = pickle.load(f)\n",
    "        print(ff)\n",
    "\n",
    "    for lib in reversed(libs):\n",
    "\n",
    "        d = summary_dict[lib][\"depth_df\"]\n",
    "        d[\"Direction\"] = d.index.str.split(\"_\").str[1].str.strip()\n",
    "        d.index = d.index.str.split(\"_\").str[0]\n",
    "        d.rename({\"Factors\":\"Configurations\"}, axis=1, inplace=True)\n",
    "        d[\"Combined FDR\"] = summary_dict[lib][\"summary_df\"].loc[d.index,(\"Combined\",\"nan\",\"Combined FDR\")]\n",
    "        d.sort_values(by=[\"Depth\",\"Combined FDR\"], ascending=[False,True], inplace=True)\n",
    "        outfile = f\"../../results/{project_name}/combined/syn.depth.{lib}.{project_name}.csv\"\n",
    "        d = d[[\"Description\",\"Depth\",\"Direction\",\"Combined FDR\",\"Configurations\"]]\n",
    "        d.to_csv(outfile)\n",
    "        \n",
    "        dd = d[(d[\"Depth\"]>1) & (d[\"Combined FDR\"]< 0.05)]\n",
    "        title = f\"{lib} {project_name.split(\"Chiara.QLF.\")[1]}\"\n",
    "        make_lollipop(dd.iloc[:30], lib, project_name, ext=\"png\", max_depth = 6, title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"../../resources/Chiara/kallisto.merged.gene_counts.clean.tsv\"\n",
    "tab = pd.read_csv(f, index_col=0, sep=\"\\t\")\n",
    "tab.columns = [''.join([i for i in c[:-3] if not i.isdigit()]) + c[-3] for c in tab.columns]\n",
    "comparisons = [\"KO_SD\", \"KO_WT\", \"KO_SA\", \"SD_WT\", \"SA_WT\", \"SD_SA\"]\n",
    "for comp in comparisons:\n",
    "    mask = tab.columns.str.contains(comp.split(\"_\")[0]) | tab.columns.str.contains(comp.split(\"_\")[1]) \n",
    "    tab_c = tab[sorted(tab.columns[mask], key= lambda x: -1 if x in comp[0] else 1)]\n",
    "    tab_c.index.name = None\n",
    "    tab_c.to_csv(f\"../../resources/Chiara/kallisto.{comp}.csv\")\n",
    "tab_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_c = pd.read_csv(f\"../../resources/Chiara/kallisto.KO_WT.csv\", index_col=0)\n",
    "tab_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually inspect direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"Chiara.QLF.KO_SA\"\n",
    "lib = \"GO\"\n",
    "tab = pd.read_csv(f\"../../results/{project_name}/combined/syn.depth.GO.{project_name}.csv\", index_col=0)\n",
    "\n",
    "ix = 1\n",
    "id = tab.index[ix]\n",
    "print(tab.iloc[ix][\"Configurations\"])\n",
    "tab.iloc[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name\n",
    "conf = \"string.logFC\"\n",
    "tab2 = pd.read_csv(f\"../../results/{project_name}/syn.{conf}.{lib}.{project_name}.csv\")\n",
    "prots = tab2[tab2[\"ID\"]==id][\"matching proteins in your input (labels)\"]\n",
    "\n",
    "tab2[tab2[\"ID\"]==id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dea = pd.read_csv(f\"../../resources/Chiara/edger.qlf.lfc0.KO_SA.p1.csv\", index_col=0)\n",
    "matched=dea[dea[\"gene_name\"].isin(prots.str.split(\",\").iloc[0])]\n",
    "\n",
    "matched[\"logFC\"].hist()\n",
    "matched.sort_values(by=\"logFC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX\n",
    "\n",
    "basestring = '''\\\\begin{figure}[!htb]\n",
    "\\centering\n",
    "\\includegraphics[width=1\\\\textwidth]{Figures/SynEnrich/Supplement/bars.BLANK.pdf}\n",
    "%\\decoRule\n",
    "\\caption[Depth and number of enriched terms (PRETTYNAME).]{\\\\textbf{Intersection depth and number of enriched terms per configuration (PRETTYNAME).} Top row: Intersection depth of all significant KEGG and GO terms. Bottom row: Number of significant terms for each configuration.}\n",
    "\\label{fig:res:enrich:bars.BLANK}\n",
    "\\end{figure}\n",
    "\n",
    "\\\\begin{figure}[!htb]\n",
    "\\centering\n",
    "\\includegraphics[width=0.8\\\\textwidth]{Figures/SynEnrich/Supplement/venn.methodcomp.BLANK.pdf}\n",
    "%\\decoRule\n",
    "\\caption[Venn diagrams of enrichment methods (PRETTYNAME).]{\\\\textbf{Venn diagrams comparing results from different enrichment tools (PRETTYNAME).}}\n",
    "\\label{fig:res:enrich:venn3:BLANK}\n",
    "\\end{figure}\n",
    "\n",
    "\\\\begin{figure}[!htb]\n",
    "\\centering\n",
    "\\includegraphics[width=1\\\\textwidth]{Figures/SynEnrich/Supplement/venn.metriccomp.BLANK.pdf}\n",
    "%\\decoRule\n",
    "\\caption[Venn diagrams of ranking metrics (PRETTYNAME).]{\\\\textbf{Venn diagrams comparing results from different ranking metrics (PRETTYNAME).} }\n",
    "\\label{fig:res:enrich:venn2:BLANK}\n",
    "\\end{figure}\n",
    "\n",
    "'''\n",
    "\n",
    "finalstring = ''''''\n",
    "\n",
    "for name, prettyname in pretty_datanames.items():\n",
    "    finalstring += basestring.replace(\"BLANK\",name).replace(\"PRETTYNAME\",prettyname)\n",
    "\n",
    "print(finalstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"logFC\"\n",
    "#metric = \"neg_signed_logpval\"\n",
    "\n",
    "tool = \"gseapy\"\n",
    "#tool = \"clusterProfiler\"\n",
    "\n",
    "f = f\"../../results/permuted.Chiara.QLF.SD_SA/syn.{tool}.{metric}.GO.permuted.Chiara.QLF.SD_SA.csv\"\n",
    "tab = pd.read_csv(f, index_col=0)\n",
    "tab[\"pvalue\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = \"GO\"\n",
    "metric = \"logFC\"\n",
    "\n",
    "\n",
    "f = f\"../../resources/Liana/deg.edger.lrt.batch.unm_0.6.clean.clInh_Sncg.thresh.0.2.2024-01-19-11-47.P14.p19rc.csv\"\n",
    "df = pd.read_csv(f, index_col=0)\n",
    "\n",
    "f = f\"../../results/{project_name}/syn.string.{metric}.{lib}.{project_name}.csv\"\n",
    "f = f\"../../results/{project_name}/syn.gseapy.{metric}.{lib}.{project_name}.csv\"\n",
    "f = f\"../../results/{project_name}/syn.clusterProfiler.{metric}.{lib}.{project_name}.csv\"\n",
    "tab = pd.read_csv(f, index_col=0)\n",
    "#tab = tab[tab[\"method\"]==\"afc\"]\n",
    "tab.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = \"GO:0097553\"\n",
    "print(tab.loc[ix,\"genes mapped\"]), len(tab.loc[ix,\"matching proteins in your input (IDs)\"].split(\",\"))\n",
    "matched = tab.loc[ix,\"matching proteins in your input (labels)\"].split(\",\")\n",
    "\n",
    "common = df.index.intersection(matched)\n",
    "dfm = df.loc[common]\n",
    "\n",
    "dfm[\"logFC\"].mean(), tab.loc[ix,\"enrichmentScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab[\"enrichmentScore\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.groupby(\"method\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth vs STRING qval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = summary_dict[lib][\"depth_df\"]\n",
    "s = summary_dict[lib][\"summary_df\"]\n",
    "s = s[(\"string\",\"logFC\",\"pvalue\")].dropna()\n",
    "s = pd.DataFrame(s)\n",
    "\n",
    "common = s.index.intersection(d.index)\n",
    "s = s.loc[common]\n",
    "d = d[~d.index.duplicated()]\n",
    "s[\"Depth\"] = d.loc[common,\"Depth\"]\n",
    "sns.boxplot(data=s,x=\"Depth\",y=(\"string\",\"logFC\",\"pvalue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "f = \"../../results/met.Inh_Meis2.P14.p19rc/syn.gseapy.logFC.GO.met.Inh_Meis2.P14.p19rc.csv\"\n",
    "tab_gse = pd.read_csv(f, index_col=0)\n",
    "\n",
    "f = \"../../results/met.Inh_Meis2.P14.p19rc/syn.clusterProfiler.logFC.GO.met.Inh_Meis2.P14.p19rc.csv\"\n",
    "tab_clu = pd.read_csv(f, index_col=0)\n",
    "\n",
    "f = \"../../results/met.Inh_Meis2.P14.p19rc/syn.string.logFC.GO.met.Inh_Meis2.P14.p19rc.csv\"\n",
    "tab_str = pd.read_csv(f, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 30\n",
    "range = (0,200)\n",
    "\n",
    "tab_gse[\"Lead_genes\"].str.split(\";\").apply(lambda x: len(x)).hist(label=\"GSEApy\", bins=bins, range=range, alpha=0.5)\n",
    "\n",
    "tab_clu[\"core_enrichment\"].str.split(\"/\").apply(lambda x: len(x)).hist(label=\"ClusterProfiler\", bins=bins, range=range, alpha=0.5)\n",
    "\n",
    "tab_str[\"matching proteins in your input (labels)\"].str.split(\",\").apply(lambda x: len(x)).hist(label=\"STRING\", bins=bins, range=range, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gseapy as gp\n",
    "tab = tab.rename({\"Description\":\"Term\"}, axis=1)\n",
    "nodes, edges = gp.enrichment_map(tab, column=\"qvalue\", cutoff=0.05, top_term=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "G = nx.from_pandas_edgelist(edges,\n",
    "                            source='src_idx',\n",
    "                            target='targ_idx',\n",
    "                            edge_attr=['jaccard_coef', 'overlap_coef', 'overlap_genes'])\n",
    "\n",
    "# Add missing node if there is any\n",
    "for node in nodes.index:\n",
    "    if node not in G.nodes():\n",
    "        G.add_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# init node cooridnates\n",
    "pos=nx.layout.spiral_layout(G)\n",
    "#node_size = nx.get_node_attributes()\n",
    "# draw node\n",
    "nx.draw_networkx_nodes(G,\n",
    "                       pos=pos,\n",
    "                       cmap=plt.cm.RdYlBu,\n",
    "                       node_color=list(nodes.NES),\n",
    "                       node_size=list(nodes.Hits_ratio *1000))\n",
    "# draw node label\n",
    "nx.draw_networkx_labels(G,\n",
    "                        pos=pos,\n",
    "                        labels=nodes.Term.to_dict())\n",
    "# draw edge\n",
    "edge_weight = nx.get_edge_attributes(G, 'jaccard_coef').values()\n",
    "nx.draw_networkx_edges(G,\n",
    "                       pos=pos,\n",
    "                       width=list(map(lambda x: x*10, edge_weight)),\n",
    "                       edge_color='#CDDBD4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GO Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_gs = pd.read_csv(f\"../../results/{project_name}/syn.gseapy.s2n.GO.met.Inh_Meis2.P14.p19rc.csv\", index_col=0)\n",
    "tab_cl = pd.read_csv(f\"../../results/{project_name}/syn.clusterProfiler.s2n.GO.met.Inh_Meis2.P14.p19rc.csv\", index_col=0)\n",
    "tab_st = pd.read_csv(f\"../../results/{project_name}/syn.string.s2n.GO.met.Inh_Meis2.P14.p19rc.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_gs = tab_gs[tab_gs[\"qvalue\"]<0.05]\n",
    "sig_cl = tab_cl[tab_cl[\"qvalue\"]<0.05]\n",
    "sig_st = tab_st[tab_st[\"qvalue\"]<0.05]\n",
    "print(len(sig_gs), len(tab_gs))\n",
    "print(len(sig_cl), len(tab_cl))\n",
    "print(len(sig_st), len(tab_st))\n",
    "\n",
    "common = set.intersection(*[set(s) for s in [sig_gs.index, sig_cl.index, sig_st.index]])\n",
    "len(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common#.loc[\"GO:0098655\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i input_file\n",
    "library(clusterProfiler)\n",
    "library(org.Hs.eg.db)\n",
    "keytypes(org.Hs.eg.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%R\n",
    "# library(org.Hs.eg.db)\n",
    "# library(AnnotationDbi)\n",
    "\n",
    "# tbl <- AnnotationDbi::toTable(org.Hs.egGO2ALLEGS)\n",
    "# print(nrow(tbl))  # 3.4 million rows\n",
    "# table(tbl$Evidence)  # see http://geneontology.org/docs/guide-go-evidence-codes/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(GO.db)\n",
    "\n",
    "go_terms_all <- keys(GO.db, keytype = \"GOID\")\n",
    "\n",
    "length(go_terms_all)  # Check the number of GO terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"../../resources/Liana/deg.edger.lrt.batch.unm_0.6.clean.clInh_Sncg.thresh.0.2.2024-01-19-11-47.P14.p19rc.csv\"\n",
    "gene_converter_file = \"../../results/met.Inh_Meis2.P14.p19rc/gene_converter.csv\"\n",
    "\n",
    "org = \"../../resources/Ontologies/GO_Org_Mm.gmt\"\n",
    "enr = \"../../resources/Ontologies/GO_Enrichr_2023.gmt\"\n",
    "\n",
    "metric = \"s2n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i infile,gene_converter_file,org,enr,metric\n",
    "source(\"../scripts/run_clusterprofiler.R\")\n",
    "set.seed(123) \n",
    "\n",
    "df <- read.csv(infile, row.names = 1)\n",
    "keytype = \"SYMBOL\"\n",
    "\n",
    "run <- function(gmt_file) {\n",
    "\n",
    "    df <- convert_df(df, keytype, gene_converter_file)\n",
    "\n",
    "    geneList <- df[[metric]]\n",
    "    names(geneList) <- toupper(df$SYMBOL)\n",
    "    geneList = sort(geneList, decreasing = TRUE)\n",
    "\n",
    "    gmt <- read.gmt(gmt_file)\n",
    "    gsea <- GSEA(geneList = geneList,\n",
    "                    TERM2GENE=gmt, \n",
    "                    minGSSize    = 10,\n",
    "                    maxGSSize    = 500,\n",
    "                    pvalueCutoff = 1,\n",
    "                    eps = 0,\n",
    "                    seed = TRUE,\n",
    "                    verbose = FALSE)\n",
    "\n",
    "    gsea\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "df <- convert_df(df, keytype, gene_converter_file)\n",
    "geneList <- df[[metric]]\n",
    "names(geneList) <- toupper(df$SYMBOL)\n",
    "geneList = sort(geneList, decreasing = TRUE)\n",
    "head(geneList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "gorg <- run(org) \n",
    "genr <- run(enr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "write.csv(gorg, \"../../results/test/clusterprofiler.gorg.met_inh.p14.csv\")\n",
    "write.csv(genr, \"../../results/test/clusterprofiler.genr.met_inh.p14.csv\")\n",
    "\n",
    "sig.org <- gorg[gorg$qvalue<0.05]\n",
    "sig.enr <- genr[genr$qvalue<0.05]\n",
    "print(nrow((sig.org)))\n",
    "print(nrow((sig.enr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_dir = os.path.abspath(os.path.join(os.getcwd(), \"../scripts\"))\n",
    "sys.path.append(scripts_dir)\n",
    "workflows_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(workflows_dir)\n",
    "\n",
    "from scripts.run_gseapy import run_gseapy_multi\n",
    "\n",
    "tab = pd.read_csv(infile, index_col=0)\n",
    "if tab.index.name != \"SYMBOL\":\n",
    "    gene_table = pd.read_csv(gene_converter_file, index_col=0)\n",
    "    tab[keytype] = tab.index\n",
    "    tab = tab.merge(gene_table, how='left', on=keytype)\n",
    "    tab.dropna(axis=0, inplace=True)\n",
    "    tab.set_index(\"SYMBOL\", inplace=True)\n",
    "tab.index = tab.index.str.upper() # Enrichr supports only upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import run_gseapy\n",
    "import importlib\n",
    "importlib.reload(run_gseapy)\n",
    "from scripts.run_gseapy import run_gseapy_multi\n",
    "\n",
    "outfile = \"../../results/test/gseapy.genr.met_inh.p14.csv\"\n",
    "ontologies = [\"GO_Biological_Process_2023\" ,\"GO_Cellular_Component_2023\",\"GO_Molecular_Function_2023\"]\n",
    "#run_gseapy_multi(tab, metric=\"s2n\", ontologies=ontologies, organismKEGG=\"mmu\", outfile=outfile)\n",
    "\n",
    "outfile = \"../../results/test/gseapy.gorg.met_inh.p14.csv\"\n",
    "run_gseapy_multi(tab, metric=\"s2n\", organismKEGG=\"mmu\", outfile=outfile, \n",
    "                gmt_file=\"../../resources/Ontologies/GO_org_Mm.gmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsgorg = pd.read_csv( \"../../results/test/gseapy.gorg.met_inh.p14.csv\", index_col=0)\n",
    "gsgenr = pd.read_csv( \"../../results/test/gseapy.genr.met_inh.p14.csv\", index_col=0)\n",
    "clgorg = pd.read_csv( \"../../results/test/clusterprofiler.gorg.met_inh.p14.csv\", index_col=0)\n",
    "clgenr = pd.read_csv( \"../../results/test/clusterprofiler.genr.met_inh.p14.csv\", index_col=0)\n",
    "string = pd.read_csv(\"../../results/met.Inh_Meis2.P14.p19rc/syn.string.s2n.GO.met.Inh_Meis2.P14.p19rc.csv\", index_col=0)\n",
    "clgenr.index = \"GO:\" + clgenr.index.str.split(\"GO:\").str[-1].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qval = 0.05\n",
    "sgsgorg = gsgorg[gsgorg[\"qvalue\"] < qval]\n",
    "sgsgenr = gsgenr[gsgenr[\"qvalue\"] < qval]\n",
    "sclgorg = clgorg[clgorg[\"qvalue\"] < qval]\n",
    "sclgenr = clgenr[clgenr[\"qvalue\"] < qval]\n",
    "sstring = string[string[\"qvalue\"] < qval]\n",
    "\n",
    "print(len(sgsgorg), len(sgsgenr), len(sclgorg), len(sclgenr), len(string))\n",
    "\n",
    "!wc $org -l\n",
    "!wc $enr -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from venny4py.venny4py import *\n",
    "\n",
    "sets = {\"GSEApy Org.Mm\": set(sgsgorg.index),\n",
    "        \"GSEApy Enrichr\": set(sgsgenr.index),\n",
    "        \"ClusterProfiler Org.Mm\": set(sclgorg.index),\n",
    "        \"ClusterProfiler Enrichr\": set(sclgenr.index)}\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "venny4py(sets=sets, asax=ax)\n",
    "ax.set_title(f\"GO Terms Inh_Meis2 P14 {metric} {qval:.0%} FDR\", fontsize=8)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(f\"../../results/test/venn4.alpha{int(100*qval)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2, venn3\n",
    "\n",
    "fig, ax= plt.subplots(2,2, figsize=(8,8))\n",
    "ax=ax.flatten()\n",
    "\n",
    "gs_sets = [v for k,v in sets.items() if \"GSEA\" in k]\n",
    "gs_labels = [k for k,v in sets.items() if \"GSEA\" in k]\n",
    "venn2(gs_sets, gs_labels, ax=ax[0])\n",
    "\n",
    "gs_sets = [v for k,v in sets.items() if \"Cluster\" in k]\n",
    "gs_labels = [k for k,v in sets.items() if \"Cluster\" in k]\n",
    "venn2(gs_sets, gs_labels, ax=ax[1])\n",
    "\n",
    "gs_sets = [v for k,v in sets.items() if \"Org\" in k]\n",
    "gs_labels = [k for k,v in sets.items() if \"Org\" in k]\n",
    "venn2(gs_sets, gs_labels, ax=ax[2])\n",
    "\n",
    "gs_sets = [v for k,v in sets.items() if \"Enri\" in k]\n",
    "gs_labels = [k for k,v in sets.items() if \"Enri\" in k]\n",
    "venn2(gs_sets, gs_labels, ax=ax[3])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.suptitle(f\"GO Terms Inh_Meis2 P14 {metric} {qval:.0%} FDR\", fontsize=18)\n",
    "fig.savefig(f\"../../results/test/venn2.alpha{int(100*qval)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upsetplot import from_contents\n",
    "from upsetplot import UpSet\n",
    "\n",
    "up = from_contents({\"STRING\": sstring.index,\n",
    "                    \"GSEApy Org\": sgsgorg.index,\n",
    "                    \"GSEApy Enr\": sgsgenr.index,\n",
    "                    \"Cluster Org\": sclgenr.index,\n",
    "                    \"Cluster Enr\": sclgenr.index})\n",
    "\n",
    "ax_dict = UpSet(up, subset_size=\"count\", sort_by=\"cardinality\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = set.intersection(*[v for v in sets.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genes_from_gmt(gmt_filepath: str, go_term: str) -> list:\n",
    "    genes = []\n",
    "    with open(gmt_filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            term = parts[0]\n",
    "            if go_term in term:\n",
    "                genes = parts[2:]  # genes start from the 3rd column\n",
    "                break\n",
    "    return genes\n",
    "\n",
    "# Example usage\n",
    "sizeorg, sizeenr, jaccs = [],[],[]\n",
    "for i in range(100):\n",
    "    go_term = list(common)[i]\n",
    "    genes_org = get_genes_from_gmt(org, go_term)\n",
    "    genes_enr = get_genes_from_gmt(enr, go_term)\n",
    "\n",
    "    inter = set(genes_org).intersection(genes_enr)\n",
    "    union = set(genes_org).union(genes_enr)\n",
    "    sizeorg.append(len(genes_org))\n",
    "    sizeenr.append(len(genes_enr))\n",
    "    if len(union):\n",
    "        jaccs.append(len(inter)/len(union))\n",
    "\n",
    "np.mean(jaccs), np.std(jaccs), np.mean(sizeorg), np.mean(sizeenr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import run_gseapy\n",
    "import importlib\n",
    "importlib.reload(run_gseapy)\n",
    "from scripts.run_gseapy import run_gseapy_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRING annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = \"../../resources/Ontologies/9606.protein.enrichment.terms.v12.0.tsv\" #human \n",
    "f = \"../../resources/Ontologies/10090.protein.enrichment.terms.v12.0.txt\" #mouse\n",
    "\n",
    "tab = pd.read_csv(f, sep=\"\\t\")\n",
    "\n",
    "orgid = os.path.basename(f).split(\".\")[0]\n",
    "orgid_dict = {\"9606\": \"human\", \"10090\": \"mouse\"}\n",
    "species = orgid_dict[orgid]\n",
    "orgid, species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabgroup = tab.groupby(\"term\").count()\n",
    "print(len(group))\n",
    "group = group[(500>group[\"#string_protein_id\"])&(group[\"#string_protein_id\"]>10)]\n",
    "group.sort_values(by=\"#string_protein_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(scripts.utils)\n",
    "\n",
    "from scripts.utils import create_string_gmt\n",
    "\n",
    "outfile = f'../../resources/Ontologies/GO_STRING_{species}.test.gmt'\n",
    "create_string_gmt(f, outfile, orgid, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot2symbol_file = f\"../../resources/Ontologies/prot2symbol.{species}.csv\"\n",
    "gene_symbols = pd.read_csv(prot2symbol_file, index_col=0, header=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gmt(gmt_file):\n",
    "    gene_sets = []\n",
    "    with open(gmt_file) as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                description = parts[1]\n",
    "                genes = parts[2:]\n",
    "                gene_sets.append({'Description': description, 'genes': genes})\n",
    "    return pd.DataFrame(gene_sets)\n",
    "\n",
    "outfile = f'../../resources/Ontologies/GO_STRING_{species}.test.gmt'\n",
    "gmt = read_gmt(outfile)\n",
    "gmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.utils\n",
    "import importlib\n",
    "importlib.reload(scripts.utils)\n",
    "\n",
    "from scripts.utils import ensp_to_gene_symbol\n",
    "\n",
    "# Example usage\n",
    "#gene_symbols = ensp_to_gene_symbol(protids, species=orgid_dict[orgid])\n",
    "print(gene_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab[\"#string_protein_id\"] = tab[\"#string_protein_id\"].str.replace(\"10090.\",\"\")\n",
    "tabgo = tab[tab[\"term\"].str.startswith(\"GO:\")]\n",
    "tabgo[\"#string_protein_id\"] = tabgo[\"#string_protein_id\"].map(gene_symbols).fillna(tabgo[\"#string_protein_id\"])\n",
    "tabgo[\"#string_protein_id\"] = tabgo[\"#string_protein_id\"].str.upper()\n",
    "\n",
    "from scripts.utils import create_gmt\n",
    "create_gmt(tabgo, f'../../resources/Ontologies/GO_STRING_{orgid_dict[orgid]}.gmt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library_ = \"../../resources/Ontologies/GO_STRING_mouse.gmt\"\n",
    "\n",
    "TERM2NAME <- read.table(library_, sep = \"\\t\", header = FALSE, colClasses = c(\"character\", \"character\", \"NULL\"), fill = TRUE, stringsAsFactors = FALSE)    \n",
    "\n",
    "TERM2NAME = TERM2NAME[c(\"V1\",\"V2\")]\n",
    "head(TERM2NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.utils\n",
    "import importlib\n",
    "importlib.reload(scripts.utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichr subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichr = \"../../resources/Ontologies/GO_Enrichr_2023.gmt\"\n",
    "\n",
    "def read_enrichr(gmt_file):\n",
    "    gene_sets = []\n",
    "    with open(gmt_file) as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                description = parts[0]\n",
    "                genes = parts[2:] # 2nd col is empty?\n",
    "                gene_sets.append({'Description': description, 'Genes': genes})\n",
    "    df = pd.DataFrame(gene_sets)\n",
    "    df.index = \"GO:\" + df[\"Description\"].str.split(\"\\(GO:\").str[1].str[:-1]\n",
    "    df.index.name = \"ID\"\n",
    "    df[\"Description\"] = df[\"Description\"].str.split(\"\\(GO:\").str[0]\n",
    "    return df\n",
    "\n",
    "enrichr_df = read_enrichr(enrichr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import read_gmt\n",
    "\n",
    "string = read_gmt(\"../../resources/Ontologies/GO_STRING_mouse.gmt\")\n",
    "org = read_gmt(\"../../resources/Ontologies/GO_Org_Mm.gmt\")\n",
    "\n",
    "s = set(string[\"ID\"])\n",
    "e = set(enrichr_df.index)\n",
    "o = set(org[\"ID\"])\n",
    "\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "venn3([s,e,o],set_labels=[\"STRING v12\",\"Enrichr 2023\",\"Org.Mm.eg.db\"], ax=ax)\n",
    "ax.set(title=\"GO Annotations\")\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "venn2([s,e],set_labels=[\"STRING v12\",\"Enrichr 2023\"], ax=ax)\n",
    "ax.set(title=\"GO Annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "project_name = \"met.Inh_Meis2.P14\"\n",
    "\n",
    "with open(f\"/home/peter/Work/OmicsPhD/SynEnrich/results/{project_name}/combined/syn.summary_dict.{project_name}.txt\", \"rb\") as f:\n",
    "    sd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_df = sd[\"GO_STRING_mouse\"][\"depth_df\"]\n",
    "depth_df[\"Depth\"].value_counts().sort_index()\n",
    "depth_df[\"ID\"] = depth_df.index\n",
    "depth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "species = iris.pop(\"species\")\n",
    "lut = dict(zip(species.unique(), \"rbg\"))\n",
    "row_colors = species.map(lut)\n",
    "sns.clustermap(iris, row_colors=row_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "semsim <-function(depth_df, ont, qval, org, measure=\"Wang\", savepath = \"\") {\n",
    "    tab <- depth_df[depth_df$ONTOLOGY == ont,]\n",
    "    terms <- tab[tab[[\"Combined.FDR\"]] < qval, ]$ID\n",
    "    godata_ont <- load_godata(org, ont)\n",
    "    sim_matrix <- mgoSim(terms, terms, semData=godata_ont, measure=measure, combine=NULL)\n",
    "    \n",
    "    if (savepath != \"\") {\n",
    "        dir.create(dirname(savepath), recursive = TRUE, showWarnings = FALSE)\n",
    "        write.csv(sim_matrix, savepath)\n",
    "    }\n",
    "\n",
    "    return(as.data.frame(sim_matrix))\n",
    "}\n",
    "\n",
    "get_semsim <- function(depth_df_path, ont, qval, org, measure=\"Wang\", savepath=\"\") {\n",
    "    depth_df <- read.csv(depth_df_path)\n",
    "    names(depth_df)[names(depth_df) == 'X'] <- 'ID'\n",
    "    sim_matrix <- semsim(depth_df, ont, qval, org, measure=\"Wang\", savepath=savepath)\n",
    "}\n",
    "\n",
    "for (subont in c(\"BP\",\"CC\",\"MF\")) {\n",
    "    print(subont)\n",
    "}\n",
    "#sim_matrix <- get_semsim(\"/home/peter/Work/OmicsPhD/SynEnrich/results/met.Inh_Meis2.P14/combined/syn.depth.GO_STRING_mouse.met.Inh_Meis2.P14.csv\",\n",
    "                        #\"BP\",1, \"org.Mm.eg.db\",\n",
    "                        #savepath=\"/home/peter/Work/OmicsPhD/SynEnrich/results/met.Inh_Meis2.P14/.cache/sim_matrx_BP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.clustering\n",
    "import importlib\n",
    "importlib.reload(scripts.clustering)\n",
    "\n",
    "from scripts.clustering import append_GO_clusters_to_depth_df\n",
    "\n",
    "append_GO_clusters_to_depth_df(depth_df, figpath =\"\", lib = \"GO_STRING_Mouse\", org=\"Org.Mm.eg.db\",\n",
    "                               project_name=\"test\", qval=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = depth_df[depth_df[\"Enrichr\"]]\n",
    "sub[\"Depth\"].value_counts().sort_index()\n",
    "#sub[sub[\"Description\"].str.contains(\"plasma membrane\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(clusterProfiler)\n",
    "library(GOSemSim)\n",
    "library(org.Mm.eg.db, lib.loc = \"../../.snakemake/conda/4e0cf237b734212e0547ba5be5ebe049_/lib/R/library\")\n",
    "#library(org.Mm.eg.db, lib.loc = \".snakemake/conda/4e0cf237b734212e0547ba5be5ebe049_/lib/R/library\")\n",
    "\n",
    "load_godata <- function(org, ont) {\n",
    "  \n",
    "  # Create the cache directory if it doesn't exist\n",
    "  cache_dir <- \"../../resources/.cache\"\n",
    "  if (!dir.exists(cache_dir)) {\n",
    "    dir.create(cache_dir, recursive = TRUE)\n",
    "  }\n",
    "  \n",
    "  # Define the cache file path\n",
    "  cache_file <- file.path(cache_dir, paste0(\"godata_\", org, \"_\", ont, \"_cache.rds\"))\n",
    "  \n",
    "  # Check if cache exists\n",
    "  if (file.exists(cache_file)) {\n",
    "    message(\"Loading cached godata...\")\n",
    "    godata_obj <- readRDS(cache_file)\n",
    "  } else {\n",
    "    message(\"Cache not found. Running godata()...\")\n",
    "    godata_obj <- godata(org, ont = ont)\n",
    "    saveRDS(godata_obj, cache_file)\n",
    "  }\n",
    "  \n",
    "  return(godata_obj)\n",
    "}\n",
    "\n",
    "\n",
    "semsim <-function(depth_df, ont, qval, org, measure=\"Wang\") {\n",
    "\n",
    "    tab <- depth_df[depth_df$ONTOLOGY == ont,]\n",
    "    terms <- tab[tab[[\"Combined FDR\"]] < qval, ]$ID\n",
    "\n",
    "    godata_ont <- load_godata(org, ont)\n",
    "\n",
    "    sim_matrix <- mgoSim(terms, terms, semData=godata_ont, measure=measure, combine=NULL)\n",
    "\n",
    "    #row.names(sim_matrix = terms)\n",
    "    #col.names(sim_matrix = terms)\n",
    "\n",
    "    return(as.data.frame(sim_matrix))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i depth_df -o sim_matrix_BP,sim_matrix_CC,sim_matrix_MF\n",
    "\n",
    "qval <- 0.05\n",
    "\n",
    "sim_matrix_BP <- semsim(depth_df, ont=\"BP\", qval=qval, org=\"org.Mm.eg.db\", measure=\"Wang\")\n",
    "sim_matrix_MF <- semsim(depth_df, ont=\"MF\", qval=qval, org=\"org.Mm.eg.db\", measure=\"Wang\")\n",
    "sim_matrix_CC <- semsim(depth_df, ont=\"CC\", qval=qval, org=\"org.Mm.eg.db\", measure=\"Wang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.clustering\n",
    "import importlib\n",
    "importlib.reload(scripts.clustering)\n",
    "\n",
    "from scripts.clustering import hierarchical_clustering\n",
    "from scripts.clustering import find_optimal_threshold\n",
    "\n",
    "def get_go_clusters(depth_df):\n",
    "\n",
    "    thresh = find_optimal_threshold(sim_matrix)\n",
    "    heat_BP, cluster_df_BP = hierarchical_clustering(sim_matrix, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = depth_df\n",
    "d[\"GO_Cluster\"] = \"\"\n",
    "d.loc[cluster_df_BP.index, \"GO_Cluster\"] = cluster_df_BP[\"GO_Cluster\"].astype(\"str\") + \"_BP\"\n",
    "\n",
    "d['Top_GO'] = d.groupby('GO_Cluster')['Combined FDR'].transform(lambda x: x == x.min())\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "venn3((set(d.index), set(d[d[\"Enrichr\"]].index), set(d[d[\"Top_GO\"]].index)),\n",
    "      set_labels=[\"All\",\"Enrichr\",\"GOSemSim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ClusterProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(clusterProfiler)\n",
    "\n",
    "convert_df <- function(df, keytype_in, keytype_target, gene_converter_file) {\n",
    "\n",
    "  if (keytype_target %in% names(df)) return(df)\n",
    "  df[[keytype_in]] <- row.names(df)\n",
    "  ids <- read.csv(gene_converter_file)\n",
    "  df <- merge(df, ids, by = keytype_in, all.x = TRUE)\n",
    "  print(paste(\"Before\",nrow(df)))\n",
    "  df <- na.omit(df)\n",
    "  print(paste(\"After\",nrow(df)))\n",
    "  return(df)\n",
    "}\n",
    "\n",
    "set.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o outfile\n",
    "\n",
    "\n",
    "outfile <- \"../../results/test/cluster.csv\"\n",
    "df <- read.csv(\"../../resources/Liana/P90/deg.edger.lrt.batch.unm_0.6.clean.clExc2_ML.thresh.0.2.2024-01-22-17-42.P90.p19rc.csv\", row.names=1)\n",
    "converter <- \"../../results/met.Exc2_ML.P90/gene_converter.csv\"\n",
    "metric <- \"s2n\"\n",
    "library_ <- \"../../resources/Ontologies/GO_STRING_mouse.gmt\"\n",
    "minGSSize <- 10\n",
    "maxGSSize <- 500\n",
    "keytype_gmt <- \"SYMBOL\"\n",
    "\n",
    "df <- convert_df(df, \"SYMBOL\", keytype_gmt, converter)\n",
    "print(paste0(\"ClusterProfiler target\", outfile))\n",
    "\n",
    "start_time <- Sys.time()\n",
    "\n",
    "geneList <- df[[metric]]\n",
    "\n",
    "gmt <- read.gmt(library_)\n",
    "gmt$gene <- toupper(gmt$gene)\n",
    "\n",
    "TERM2NAME <- read.table(library_, sep = \"\\t\", header = FALSE, fill = TRUE, stringsAsFactors = FALSE)\n",
    "TERM2CAT = TERM2NAME[c(\"V1\",\"V2\")]\n",
    "TERM2NAME = TERM2NAME[c(\"V1\",\"V3\")]\n",
    "colnames(TERM2NAME) <- c(\"term\", \"description\")\n",
    "colnames(TERM2CAT) <- c(\"term\", \"ONTOLOGY\")\n",
    "\n",
    "# Keep only unique term and description pairs for TERM2NAME\n",
    "TERM2NAME <- unique(TERM2NAME[, c(\"term\", \"description\")])\n",
    "TERM2CAT <- unique(TERM2CAT[, c(\"term\", \"ONTOLOGY\")])\n",
    "\n",
    "# Ensure the terms in TERM2NAME are uppercase to match TERM2GENE\n",
    "TERM2NAME$term <- toupper(TERM2NAME$term)\n",
    "\n",
    "names(geneList) <- toupper(df[[keytype_gmt]])\n",
    "geneList = sort(geneList, decreasing = TRUE)\n",
    "\n",
    "ego3 <- GSEA(geneList     = geneList,\n",
    "            TERM2GENE = gmt <- read.gmt(library_),\n",
    "            TERM2NAME = TERM2NAME,\n",
    "            minGSSize    = minGSSize,\n",
    "            maxGSSize    = maxGSSize,\n",
    "            pvalueCutoff = 1,\n",
    "            eps = 0,\n",
    "            seed = TRUE,\n",
    "            verbose = FALSE)\n",
    "\n",
    "ego3 <- merge(ego3, TERM2CAT, by.x = \"ID\", by.y = \"term\", all.x = TRUE, row.names = \"ID\")\n",
    "write.csv(ego3,outfile, row.names=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.read_csv(str(outfile[0]), index_col=0)\n",
    "\n",
    "sig = tab[tab[\"qvalue\"]<0.05]\n",
    "sig.sort_values(by=\"pvalue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.read_csv(\"/home/peter/Work/OmicsPhD/SynEnrich/results/met.Exc6_ML.P90/syn.gseapy.logFC.GO_STRING_mouse.met.Exc6_ML.P90.csv\", index_col=0)\n",
    "\n",
    "tab.hist(\"pvalue\",bins=20)\n",
    "\n",
    "tab[tab[\"qvalue\"]<10.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEGG Human vs Mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "orghs <- suppressMessages(library(org.Hs.eg.db, lib.loc = \"../../.snakemake/conda/4e0cf237b734212e0547ba5be5ebe049_/lib/R/library\"))\n",
    "orgmm <- suppressMessages(library(org.Mm.eg.db, lib.loc = \"../../.snakemake/conda/4e0cf237b734212e0547ba5be5ebe049_/lib/R/library\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R    \n",
    "\n",
    "run_kEGG <- function(df, metric, outfile, organism.KEGG, minGSSize=10, maxGSSize=500) {\n",
    "    geneList <- df[[metric]]\n",
    "    names(geneList) <- df$ENTREZID\n",
    "    geneList = sort(geneList, decreasing = TRUE)\n",
    "    print(head(geneList))\n",
    "    print(\"Running KEGG...\")\n",
    "    print(organism.KEGG)\n",
    "    kegg <- gseKEGG(geneList     = geneList,\n",
    "                  organism        = organism.KEGG,\n",
    "                  minGSSize    = minGSSize,\n",
    "                  maxGSSize    = maxGSSize,\n",
    "                  pvalueCutoff = 1,\n",
    "                  eps = 0,\n",
    "                  seed = TRUE,\n",
    "                  verbose = FALSE)\n",
    "    write.csv(kegg,outfile)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o outfile_hs,outfile_mm\n",
    "\n",
    "outfile_hs <- \"../../results/test/cluster_kegg_hs.csv\"\n",
    "outfile_mm <- \"../../results/test/cluster_kegg_mm.csv\"\n",
    "\n",
    "df <- read.csv(\"../../resources/Liana/P90/deg.edger.lrt.batch.unm_0.6.clean.clExc1_SF.thresh.0.2.2024-01-22-17-42.P90.p19rc.csv\", row.names=1)\n",
    "project_name <- \"met.Exc1_SL.P90\"\n",
    "converter <- paste0(\"../../results/\", project_name, \"/gene_converter.csv\")\n",
    "\n",
    "\n",
    "metric <- \"s2n\"\n",
    "minGSSize <- 10\n",
    "maxGSSize <- 500\n",
    "keytype_gmt <- \"SYMBOL\"\n",
    "\n",
    "df <- convert_df(df, \"SYMBOL\", keytype_gmt, converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#run_kEGG(df, metric, outfile_hs, \"hsa\", minGSSize, maxGSSize) # no gene can be mapped\n",
    "run_kEGG(df, metric, outfile_mm, \"mmu\", minGSSize, maxGSSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.read_csv(str(outfile_mm[0]), index_col=0)\n",
    "tab[tab[\"Description\"].str.contains(\"Cardiac\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SynEnrich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
